"""
GetGSA: AI + RAG System
Main FastAPI application with document ingestion, AI classification, and RAG-based analysis.
"""

import os
import uuid
import time
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from fastapi import FastAPI, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

from src.models import IngestRequest, IngestResponse, AnalyzeResponse
from src.document_processor import DocumentProcessor
from src.rag_system import RAGSystem
from src.ai_service import EnterpriseAIService
from src.pii_redactor import PIIRedactor
from src.storage import DocumentStorage
import logging

# Configure professional logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def convert_to_json_serializable(obj):
    """Convert objects to JSON serializable format"""
    from src.models import ClassificationResult, ChecklistItem, Citation
    
    if isinstance(obj, ClassificationResult):
        return {
            "document_name": obj.document_name,
            "predicted_type": obj.predicted_type,
            "confidence": obj.confidence,
            "abstained": obj.abstained,
            "reason": obj.reason
        }
    elif isinstance(obj, ChecklistItem):
        return {
            "rule_id": obj.rule_id,
            "description": obj.description,
            "required_ok": obj.required_ok,
            "problems": obj.problems,
            "evidence": obj.evidence
        }
    elif isinstance(obj, Citation):
        return {
            "rule_id": obj.rule_id,
            "chunk": obj.chunk,
            "relevance_score": obj.relevance_score
        }
    elif isinstance(obj, dict):
        return {k: convert_to_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_json_serializable(item) for item in obj]
    else:
        return obj

# Initialize FastAPI app
app = FastAPI(
    title="GetGSA: AI + RAG System",
    description="Smart GSA document processing with AI classification and RAG-based policy checking",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize enterprise-grade services
document_processor = DocumentProcessor()
rag_system = RAGSystem()
ai_service = EnterpriseAIService()  # üöÄ Enhanced AI with multi-provider support
pii_redactor = PIIRedactor()
document_storage = DocumentStorage()

# Enterprise configuration
ENTERPRISE_CONFIG = {
    "version": "2.0.0-enterprise",
    "features": ["multi-ai-providers", "advanced-caching", "performance-metrics", "health-monitoring"],
    "max_concurrent_requests": 50,
    "cache_enabled": True,
    "monitoring_enabled": True
}

# Global state for demo (in production, use proper database)
current_analysis_data = {}

@app.on_event("startup")
async def startup_event():
    """Initialize RAG system with GSA Rules Pack"""
    # RAG system initializes automatically in __init__ now
    print("GetGSA system initialized successfully")

@app.get("/", response_class=HTMLResponse)
async def root():
    """Serve the main UI"""
    return FileResponse("static/index.html")

@app.get("/healthz")
async def health_check():
    """Basic health check endpoint"""
    return {"ok": True, "timestamp": datetime.utcnow().isoformat()}

@app.get("/health")
async def enterprise_health_check():
    """üè• Comprehensive enterprise health check with AI provider status"""
    try:
        ai_health = await ai_service.health_check()
        system_metrics = ai_service.get_performance_metrics()
        
        return {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "version": ENTERPRISE_CONFIG["version"],
            "ai_providers": ai_health["providers"],
            "system_health": system_metrics["system_health"],
            "performance": {
                "avg_response_time": system_metrics["performance"]["average_response_time"],
                "success_rate": system_metrics["performance"]["overall_success_rate"],
                "cache_hit_rate": system_metrics["caching"]["cache_hit_rate"]
            },
            "features": ENTERPRISE_CONFIG["features"]
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "timestamp": datetime.utcnow().isoformat(),
            "error": str(e)
        }

@app.get("/metrics")
async def get_performance_metrics():
    """üìä Enterprise performance metrics endpoint"""
    try:
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "metrics": ai_service.get_performance_metrics(),
            "config": ENTERPRISE_CONFIG
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Metrics collection failed: {str(e)}")

@app.post("/ingest", response_model=IngestResponse)
async def ingest_documents(request: IngestRequest):
    """
    Ingest and store documents with PII redaction
    """
    try:
        request_id = str(uuid.uuid4())
        doc_summaries = []
        
        for doc in request.documents:
            # Validate document size
            if len(doc.text) > 1048576:  # 1MB limit
                raise HTTPException(status_code=413, detail=f"Document {doc.name} too large")
            
            # Redact PII from the document
            redacted_text = pii_redactor.redact_text(doc.text)
            
            # Store the document
            doc_id = await document_storage.store_document(
                name=doc.name,
                original_text=doc.text,
                redacted_text=redacted_text,
                type_hint=doc.type_hint,
                request_id=request_id
            )
            
            doc_summaries.append({
                "doc_id": doc_id,
                "name": doc.name,
                "type_hint": doc.type_hint,
                "character_count": len(doc.text),
                "redacted_character_count": len(redacted_text),
                "pii_items_redacted": len(doc.text) - len(redacted_text)
            })
        
        # Store request metadata
        global current_analysis_data
        current_analysis_data[request_id] = {
            "documents": request.documents,
            "doc_summaries": doc_summaries,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return IngestResponse(
            doc_summaries=doc_summaries,
            request_id=request_id,
            message=f"Successfully ingested {len(request.documents)} documents"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ingestion failed: {str(e)}")

class AnalyzeRequest(BaseModel):
    request_id: Optional[str] = None
    analysis_type: Optional[str] = "comprehensive"
    include_checklist: bool = True
    generate_brief: bool = True
    generate_email: bool = True

@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_documents(request: Optional[AnalyzeRequest] = None):
    """
    Analyze the last ingested documents or specific request
    """
    try:
        request_id = request.request_id if request else None
        logger.info(f"üîç Starting analysis for request_id: {request_id}")
        
        # Get the most recent request if no ID provided
        if not request_id:
            if not current_analysis_data:
                logger.error("‚ùå No documents to analyze")
                raise HTTPException(status_code=400, detail="No documents to analyze. Please ingest documents first.")
            request_id = max(current_analysis_data.keys(), key=lambda k: current_analysis_data[k]["timestamp"])
            logger.info(f"üìã Using most recent request: {request_id}")
        
        if request_id not in current_analysis_data:
            logger.error(f"‚ùå Request ID not found: {request_id}")
            raise HTTPException(status_code=404, detail=f"Request ID {request_id} not found")
        
        documents = current_analysis_data[request_id]["documents"]
        processing_start = time.time()
        logger.info(f"üìÑ Processing {len(documents)} documents")
        
        # Process documents with error handling
        try:
            logger.info("üîÑ Starting document processing...")
            analysis_results = await document_processor.process_documents(documents)
            logger.info("‚úÖ Document processing complete")
        except Exception as e:
            logger.error(f"‚ùå Document processing failed: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Document processing failed: {str(e)}")
        
        # Generate RAG-based checklist with error handling
        try:
            logger.info("üß† Starting RAG checklist generation...")
            checklist = await rag_system.generate_checklist(analysis_results)
            logger.info(f"‚úÖ RAG checklist complete: {len(checklist)} items")
        except Exception as e:
            logger.error(f"‚ùå RAG checklist generation failed: {str(e)}")
            raise HTTPException(status_code=500, detail=f"RAG analysis failed: {str(e)}")
        
        # Convert analysis results to JSON serializable format
        try:
            serializable_analysis_results = convert_to_json_serializable(analysis_results)
            logger.info("‚úÖ Analysis results converted to JSON serializable format")
        except Exception as e:
            logger.error(f"‚ùå Analysis results conversion failed: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Data conversion failed: {str(e)}")
        
        # Generate AI content with enterprise features (concurrent processing)
        try:
            logger.info("ü§ñ Starting AI content generation...")
            logger.info("üîÑ Using OpenAI GPT-4 with GROQ Llama3 fallback")
            
            # Cast to proper type for AI service (Dict[str, Any])
            analysis_dict = dict(serializable_analysis_results) if isinstance(serializable_analysis_results, dict) else {}
            brief_task = ai_service.generate_negotiation_brief(analysis_dict, checklist)
            email_task = ai_service.generate_client_email(analysis_dict, checklist)
            
            # Execute AI generation concurrently for better performance
            (brief, brief_meta), (client_email, email_meta) = await asyncio.gather(
                brief_task, email_task
            )
            
            logger.info(f"‚úÖ AI content generated: Brief via {brief_meta.get('provider', 'Unknown')}, Email via {email_meta.get('provider', 'Unknown')}")
            
            # Ensure we have valid content
            if not brief or len(brief.strip()) < 50:
                logger.warning("‚ö†Ô∏è Brief content seems incomplete, using template fallback")
                brief = "Professional analysis brief generated. Please review the checklist items for detailed compliance assessment."
                
            if not client_email or len(client_email.strip()) < 50:
                logger.warning("‚ö†Ô∏è Email content seems incomplete, using template fallback")
                client_email = "Dear Client, please find attached the compliance analysis results. We have identified several items that require attention."
                
            logger.info("‚úÖ AI content generation complete")
        except Exception as e:
            logger.error(f"‚ùå AI content generation failed: {str(e)}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            
            # Use fallback content instead of failing
            logger.info("üîÑ Using fallback AI content")
            brief = "Analysis completed. Please review the compliance checklist for detailed findings and recommendations."
            client_email = "Dear Client, the document analysis has been completed. Please review the attached findings and let us know if you need any clarification."
            brief_meta = {"provider": "template-fallback", "quality_score": 5, "response_time": 0.1}
            email_meta = {"provider": "template-fallback", "quality_score": 5, "response_time": 0.1}
        
        # Get rule citations
        try:
            citations = rag_system.get_recent_citations()
            logger.info(f"üìö Retrieved {len(citations)} citations")
        except Exception as e:
            logger.error(f"‚ùå Citation retrieval failed: {str(e)}")
            citations = []
        
        # Enhanced response with enterprise metadata
        response_data = {
            "parsed": serializable_analysis_results,
            "checklist": [convert_to_json_serializable(item) for item in checklist],
            "brief": brief,
            "client_email": client_email,
            "citations": [convert_to_json_serializable(citation) for citation in citations],
            "request_id": request_id,
            "documents_analyzed": len(documents),
            "compliance_status": "Analyzed",
            "enterprise_metadata": {
                "brief_generation": brief_meta,
                "email_generation": email_meta,
                "processing_time": round(time.time() - processing_start, 2),
                "ai_providers_used": [brief_meta["provider"], email_meta["provider"]],
                "quality_scores": {
                    "brief": brief_meta.get("quality_score", 0),
                    "email": email_meta.get("quality_score", 0)
                },
                "performance_metrics": {
                    "response_time": round(time.time() - processing_start, 2),
                    "ai_provider": brief_meta["provider"],
                    "pii_protected": True
                }
            }
        }
        
        logger.info(f"üéÜ Analysis complete for {request_id}: {len(checklist)} rules checked, AI: {brief_meta['provider']}/{email_meta['provider']}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Unexpected analysis error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)